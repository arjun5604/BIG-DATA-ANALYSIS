# BIG-DATA-ANALYSIS

**COMPANY**: CODTECH IT SOLUTIONS

**NAME**: ARJUN BHAGAVAN LAGAD

**INTERN ID**: CT08HJY

**DOMAIN**: DATA ANALYSIS

**BATCH DURATION**: DECEMBER 30, 2024 TO JANUARY 30, 2025

**MENTOR NAME**: NEELA SANTHOSH

Internship Task 1: Analyzing Flipkart Big Billion Days Product Sale Data Using PySpark on Databricks

As part of my internship at Versa Controls, I undertook a data analysis task leveraging PySpark and Databricks to explore the Flipkart Big Billion Days product sale dataset, which I sourced from GitHub. This task aimed to showcase the scalability and efficiency of PySpark in handling large datasets while utilizing the collaborative and powerful capabilities of Databricks as a unified analytics platform.

Objective
The primary goal of this task was to analyze the Flipkart Big Billion Days sales data to extract meaningful insights about sales trends, product categories, pricing strategies, and customer preferences. By processing this large dataset using PySpark, I sought to demonstrate the framework's ability to handle data-intensive operations with scalability and speed. Databricks provided a seamless environment for development, collaboration, and visualization of results, making it an ideal choice for this analysis.

Dataset Overview
The dataset contains detailed information about products sold during Flipkart's Big Billion Days sale. It includes key attributes such as product names, categories, prices, discounts, ratings, and customer reviews. Additionally, there are timestamps capturing when the sales occurred, enabling temporal analysis of sales trends. The data's volume and diversity made it an excellent candidate for showcasing the power of distributed computing tools like PySpark.

Tools and Technologies
1. PySpark: A Python API for Apache Spark, PySpark enabled me to perform distributed data processing, allowing me to process the large dataset efficiently. With its extensive library of functions, I conducted data cleaning, transformation, and analysis seamlessly.

2. Databricks: This cloud-based platform provided an integrated environment for my analysis. Its ability to handle notebooks, collaborate in real-time, and execute Spark jobs simplified the workflow and enhanced productivity.

3. Visualization Libraries: I used Matplotlib and Seaborn for data visualization, integrated within Databricks notebooks to present the insights in an easy-to-understand format.

Key Steps in Analysis
1. Data Ingestion: I imported the dataset into Databricks using PySpark's capabilities to read large CSV files efficiently. The dataset was loaded into a distributed data frame for further processing.

2. Data Cleaning: This step involved handling missing values, removing duplicate entries, and ensuring consistency in data types across columns. String and numeric transformations were applied to make the data analysis-ready.

3. Exploratory Data Analysis (EDA):
Analyzed the most popular product categories during the sale.
Examined pricing trends and the effect of discounts on sales.
Investigated customer ratings and reviews to understand satisfaction levels.
Studied temporal trends to identify peak sales hours and days.

4. Feature Engineering: I created new features such as discount percentage, price-to-rating ratio, and sales frequency for a deeper understanding of sales dynamics.

5. Performance Optimization: Leveraging PySpark's parallel processing, I optimized operations such as joins, aggregations, and filters to ensure scalability and efficiency.

Insights and Findings
1. Product Trends: Certain categories, like electronics and fashion, showed higher sales volumes.

2. Discount Impact: Products with discounts between 40-60% were the most popular, suggesting an optimal discount range for customer engagement.

3. Customer Preferences: Highly rated products had a significant impact on customer buying decisions, with a positive correlation between ratings and sales.

4. Temporal Trends: The first and last days of the sale witnessed a significant spike in purchases, with late-night hours showing the highest activity.

Challenges and Solutions
Large Dataset: Processing the voluminous data required efficient partitioning strategies in PySpark.
Missing Values: Missing or inconsistent data entries were handled using PySpark's imputation and filtering techniques.
Visualization: Rendering large-scale data visualizations was challenging but was managed by aggregating data into smaller, meaningful subsets.

Conclusion

This task demonstrated the potential of PySpark and Databricks in analyzing large datasets effectively. By processing and analyzing the Flipkart Big Billion Days sales data, I gained valuable insights into customer behavior and sales patterns. This experience has not only strengthened my data analysis skills but also highlighted the importance of scalable tools in modern data-driven environments

